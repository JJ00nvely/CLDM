{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from cgllike import CLDM\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import DDPMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from dataset_eval import ImageLayout\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from data_utils import norm_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{7}\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = DDPMScheduler(num_train_timesteps=250, prediction_type='sample', clip_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_loss(predicted_box, sample, zero_count,cxcy,wh):\n",
    "    src = sample['sr']\n",
    "    src_list = []\n",
    "    \n",
    "    for i in src:\n",
    "        src_ = Image.open(i)\n",
    "        src_list.append(src_)\n",
    "    \n",
    "    box = predicted_box.cpu().numpy()\n",
    "    box = (box + 1) / 2\n",
    "    \n",
    "    match_list = []\n",
    "    size_list = []\n",
    "    for i in range(box.shape[0]):\n",
    "        img = src_list[i]\n",
    "        width, height = img.size\n",
    "        cx, cy, w, h = box[i]\n",
    "        x = int((cx - w / 2) * width)\n",
    "        y = int((cy - h / 2) * height)\n",
    "        x2 = int((cx + w / 2) * width)\n",
    "        y2 = int((cy + h / 2) * height)\n",
    "        boxes = (x, y, x2, y2)\n",
    "\n",
    "        # 이미지 크롭\n",
    "        crop = img.crop(boxes)\n",
    "        crop = np.array(crop)\n",
    "\n",
    "\n",
    "        if crop.size == 0:\n",
    "            print(f\"Warning: Crop size is zero for box {boxes}.\")\n",
    "            match_list.append(0)  \n",
    "            zero_count+=1\n",
    "            continue\n",
    "        \n",
    "        blue_channel = crop[:, :, 2]\n",
    "        blue_channel_flatten = blue_channel.flatten()\n",
    "        \n",
    "\n",
    "        match_pixel_size = np.sum(blue_channel_flatten == 128) / blue_channel_flatten.size\n",
    "        match_list.append(match_pixel_size)\n",
    "\n",
    "        if crop.size ==0:\n",
    "            size_list.append(1)\n",
    "\n",
    "        else:\n",
    "            _cx = cx*width\n",
    "            _cy = cy*height\n",
    "\n",
    "            center_point = np.array([_cx,_cy])\n",
    "            normalized_area = w*h\n",
    "\n",
    "            distances = np.linalg.norm(cxcy - center_point, axis=1)\n",
    "\n",
    "            min_index = np.argmin(distances)\n",
    "            gtw, gth =wh[min_index]\n",
    "            gtwh = gtw*gth\n",
    "            size_list.append(abs(gtwh-normalized_area))\n",
    "\n",
    "    # NaN 방지를 위해 match_list가 비어 있지 않은지 확인\n",
    "    if len(match_list) == 0:\n",
    "        print(\"Error: All crops have zero size. Returning NaN.\")\n",
    "        return float('nan')\n",
    "    \n",
    "    value = sum(match_list) / len(match_list)\n",
    "    size = sum(size_list)/ len(size_list)\n",
    "\n",
    "    print(value)\n",
    "    print('size here')\n",
    "    print(size_list)\n",
    "    return value, zero_count, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model with Seperate Param dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ImageLayout(type='92.158.10')\n",
    "dataset = DataLoader(val, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxcy = np.array([[item[0], item[1]] for item in (val.box_list)])\n",
    "wh = np.array([[item[2]/800, item[3]/600] for item in (val.box_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = []\n",
    "for i in range(0,300, 30):\n",
    "    epoch.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-19 23:31:12.564\u001b[0m \u001b[1;30mINFO    \u001b[0m \u001b[34mlogger_set:26\u001b[0m -> Loading the resnet50 encoder\n",
      "/data1/joonsm/anaconda3/envs/CLDM/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "  0%|          | 1/542 [00:05<53:27,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32808908045977014\n",
      "size here\n",
      "[1.10442973673343e-05, 0.00035742091620340937, 0.006044052730780095, 0.004717467524499322]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/542 [00:09<42:01,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6990740740740741\n",
      "size here\n",
      "[0.004091740624854962, 0.0006930742425533634, 0.005858363249339163, 0.003964810081074636]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/542 [00:14<44:25,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09230769230769231\n",
      "size here\n",
      "[0.004043336057352524, 0.008262468109931797, 0.009676968113208811, 0.004897548348565275]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/542 [00:20<46:36,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.579926575330987\n",
      "size here\n",
      "[0.0002087788433302194, 0.0033470623119423787, 0.00013034660397097473, 7.347458552879588e-05]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/542 [00:25<45:57,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7473544973544973\n",
      "size here\n",
      "[0.0005614581932779402, 0.004168895665199185, 0.007353968440927565, 0.0010816358519795662]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/542 [00:30<46:21,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9408022533022533\n",
      "size here\n",
      "[0.0007914381897232185, 4.539594566449522e-05, 0.003518863133651515, 0.00015649160300381488]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/542 [00:35<52:21,  5.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     21\u001b[0m         noise_pred \u001b[38;5;241m=\u001b[39m model(noisy_batch, timesteps\u001b[38;5;241m=\u001b[39mt)\n\u001b[0;32m---> 22\u001b[0m         bbox_pred \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnoisy_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbox\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         noisy_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bbox_pred\u001b[38;5;241m.\u001b[39mprev_sample\n\u001b[1;32m     24\u001b[0m predicted \u001b[38;5;241m=\u001b[39m bbox_pred\u001b[38;5;241m.\u001b[39mprev_sample\n",
      "File \u001b[0;32m~/anaconda3/envs/CLDM/lib/python3.9/site-packages/diffusers/schedulers/scheduling_ddpm.py:445\u001b[0m, in \u001b[0;36mDDPMScheduler.step\u001b[0;34m(self, model_output, timestep, sample, generator, return_dict)\u001b[0m\n\u001b[1;32m    443\u001b[0m         variance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m variance) \u001b[38;5;241m*\u001b[39m variance_noise\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 445\u001b[0m         variance \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_variance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_variance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m) \u001b[38;5;241m*\u001b[39m variance_noise\n\u001b[1;32m    447\u001b[0m pred_prev_sample \u001b[38;5;241m=\u001b[39m pred_prev_sample \u001b[38;5;241m+\u001b[39m variance\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src={}\n",
    "cxcy = np.array([[item[0], item[1]] for item in (val.box_list)])\n",
    "for index,value in enumerate(epoch):\n",
    "    save_path = f\"/data1/joonsm/City_Layout/log_dir/CGL[FPN50]/checkpoints/checkpoint-{value}/pytorch_model.bin\"\n",
    "    model = CLDM(use_temp=False,backbone_name='resnet50')\n",
    "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    zero_count = 0\n",
    "    batch_value = []\n",
    "    batch_size = []\n",
    "    step = 0\n",
    "    with torch.no_grad():\n",
    "        for step,batch in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "            shape = batch['box'].shape\n",
    "            noisy_batch = {'image':batch['image'].to(device),\n",
    "                    'box': torch.rand(*shape, dtype=torch.float32, device=device)}\n",
    "            for i in range(250)[::-1]:\n",
    "                t = torch.tensor([i]*shape[0], device=device)\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = model(noisy_batch, timesteps=t)\n",
    "                    bbox_pred = diffusion.step(noise_pred, t[0].detach().item(),  noisy_batch['box'], return_dict=True)\n",
    "                    noisy_batch['box'] = bbox_pred.prev_sample\n",
    "            predicted = bbox_pred.prev_sample\n",
    "            value,zero_count,size = seg_loss(predicted, batch,zero_count,cxcy,wh)\n",
    "            print(zero_count)\n",
    "            batch_value.append(value)\n",
    "            batch_size.append(size)\n",
    "            step +=1 \n",
    "\n",
    "        final = sum(batch_value)/step\n",
    "        final_size = sum(batch_size)/step\n",
    "        src[index]={'score': final, 'zero_count':zero_count, 'size':final_size}\n",
    "    del model\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
